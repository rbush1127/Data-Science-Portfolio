{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Initial Modeling and Error Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_model = pd.read_excel('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\PGA Finish Projections\\\\PGA Finish Projections_modeling data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'player', 'player_id', 'tournament_name',\n",
       "       'tournament_id', 'course', 'course_id', 'region', 'region_id', 'season',\n",
       "       'days_from_today', 'sg_putt', 'sg_arg', 'sg_app', 'sg_ott', 'sg_t2g',\n",
       "       'sg_total', 'nan', 'mc', 'strokes', 'strokes_rel_par', 'place_adj',\n",
       "       'nan_count', 'tourn_count', 'sg_putt1', 'sg_putt2', 'sg_putt3',\n",
       "       'sg_arg1', 'sg_arg2', 'sg_arg3', 'sg_app1', 'sg_app2', 'sg_app3',\n",
       "       'sg_ott1', 'sg_ott2', 'sg_ott3', 'sg_t2g1', 'sg_t2g2', 'sg_t2g3',\n",
       "       'sg_total1', 'sg_total2', 'sg_total3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, I've spent time cleaning the dataset, investigating whether there are significant differences in performance given a player's history at a particular course or tournament, and understanding to what extent recent performance impacts a player's next tournament finish.  In the next section I will use these insights as features in a model to predict future performance.\n",
    "\n",
    "Below I do a few different things:\n",
    "\n",
    "<ol>\n",
    "    <li><b>Establish a baseline of error evaluation:</b> I will use each record's prior strokes-gained data as a predictor for <em>place_adj</em>, and use the resulting <em>root-mean-squared-error (RMSE)</em> as an error baseline.  When I create a model with new features or hyperparameters, I will calculate the difference in RMSE between the baseline and the new model to see how much more of the relationship is explained (i.e. if the RMSE difference is negative after adding new features, the error has been reduced, and the new model is better).</li>\n",
    "    <li><b>Quantify the impact of including player, course, and tournament on predictive power:</b> the baseline will not care what player posted the recent strokes-gained performance, and won't care where they did it.  However, we know from our ANOVA that course and tournament are significantly correlated with a player's finish, therefore I expect these categorical features to also improve predictive power significantly.</li>\n",
    "    <li><b>Assess the improvement due to lagged strokes-gained data:</b> in the baseline case we will only use the prior tournament's performance as a predictor.  However, we know that an auto-correlation in strokes-gained performance can be observed over a period of 3 weeks is observable, in turn having a relationship with tournament finishes.  We will add lagged strokes-gained performance and assess its impact on predictive power.</li>\n",
    "</ol>\n",
    "\n",
    "For our modeling and error evaluation in this section, I'll use Scikit-Learn to fit simple 100-estimator random forest models.  Below, I have written a simple function to calculate RMSE which is used to evaluate each model's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, actual):\n",
    "    mse = mean_squared_error(predictions, actual)\n",
    "    rmse = sqrt(mse)\n",
    "    residuals = [predictions-actual]\n",
    "    return(rmse, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_model, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 23605 rows\n",
      "The test dataset has 2623 rows\n"
     ]
    }
   ],
   "source": [
    "print('The train dataset has %s rows' % (len(train.iloc[:,0])))\n",
    "print('The test dataset has %s rows' % (len(test.iloc[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Accuracy Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a list of all of the strokes-gained fields from the week prior and store it as the variable <em>rf_base_fields</em>.  As we add features in to the model, we will expand on this list and evaluate the new accuracy measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_1_fields = ['sg_putt1','sg_arg1','sg_app1','sg_ott1','sg_t2g1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestRegressor(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_base.fit(train[lag_1_fields], train['strokes_rel_par'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_base = rf_base.predict(test[lag_1_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_base, _ = rmse(predictions_base, test['strokes_rel_par'])\n",
    "base_rmse = round(np.mean(errors_base), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean-Squred Error: 7.06\n"
     ]
    }
   ],
   "source": [
    "print('Root Mean-Squred Error:', base_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real terms, a RMSE of ~28 means that we are over- or under-estimating a player's finish by 28 places in a given PGA Tour event.  Because the RMSE is weighting edge-cases more heavily than the close predictions, my first reaction is that this is likely not a terrible place to start for the most highly-ranked and consistent players in the world.  However, we will certainly look to improve upon this figure.\n",
    "\n",
    "In the <b><em>Feature Evaluation</em></b> section below, we'll see what kind of improvement we get by adding in the features we mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_evaluate(train, test, target_field, base_error, scenario_names, scenario_features, n_estimators, random_state):\n",
    "    \"\"\"\n",
    "    Take the error from the base case above and evaluate the % decrease associated with adding additional features.  The output\n",
    "    is a printed output which shows the name of the scenario, and what % change in error it drove.\n",
    "    \"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    for scenario in scenario_features:\n",
    "        \n",
    "        random_forest = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "        random_forest.fit(train[scenario], train[target_field])\n",
    "        predictions = random_forest.predict(test[scenario])\n",
    "        \n",
    "        residuals, _ = rmse(predictions, test[target_field])\n",
    "        error = np.mean(residuals)\n",
    "        error_pct_diff = round(100*(error-base_error)/base_error, 2)\n",
    "        \n",
    "        print_string = '%s: %s' % (scenario_names[i], error_pct_diff)\n",
    "        print(print_string+'%'+'\\n')\n",
    "        \n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_names = ['Player, Course, and Tournament', \n",
    "                 'Lagged Strokes-Gained Features', \n",
    "                 'Player, Course, and Tournament with Lagged Strokes-Gained']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll select only the fields we need to investigate error reduction in our scenarios listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player, Course, and Tournament\n",
    "pct_fields = ['player_id']+['course_id']+['tournament_id']+lag_1_fields\n",
    "\n",
    "# Lagged Strokes-Gained Feature\n",
    "lag_2_fields = ['sg_putt2','sg_arg2','sg_app2','sg_ott2','sg_t2g2']\n",
    "lag_3_fields = ['sg_putt3','sg_arg3','sg_app3','sg_ott3','sg_t2g3']\n",
    "\n",
    "sg_fields = lag_1_fields+lag_2_fields+lag_3_fields\n",
    "\n",
    "# Player, Course, and Tournament with Lagged Strokes-Gained\n",
    "pct_sg_fields = sg_fields+['player_id']+['tournament_id']+['course_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_features = [pct_fields, sg_fields, pct_sg_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player, Course, and Tournament: -17.53%\n",
      "\n",
      "Lagged Strokes-Gained Features: -3.13%\n",
      "\n",
      "Player, Course, and Tournament with Lagged Strokes-Gained: -16.26%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_evaluate(train, test, 'strokes_rel_par', base_rmse, scenario_names, scenario_features, n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the inclusion of features we tested in our statistical analysis before is reducing our error.  When player, course, tournament, and lagged performance are all included, we're seeing a quick 6% reduction of error.\n",
    "\n",
    "However, while it's nice to see an error reduction, in real terms that only reduces our estimated tournament finish from an average difference of ~28 places to ~26.  We're certainly on the right track, but there is more we can do to squeeze additional information out of this dataset.  In the next section, we'll see how much more we can increase our accuracy via feature engineering, another modeling approach, and hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
