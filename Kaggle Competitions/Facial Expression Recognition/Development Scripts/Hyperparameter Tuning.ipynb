{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, Conv2D, Flatten, MaxPooling2D, AveragePooling2D, LeakyReLU, concatenate\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow import clip_by_value\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "from random import sample\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\train_images.parquet').read().to_pandas()\n",
    "test_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\test_images.parquet').read().to_pandas()\n",
    "val_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\val_images.parquet').read().to_pandas()\n",
    "\n",
    "train_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\train_labels.parquet').read().to_pandas()\n",
    "test_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\test_labels.parquet').read().to_pandas()\n",
    "val_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\val_labels.parquet').read().to_pandas()\n",
    "\n",
    "train_images = np.array(train_images).reshape(train_images.shape[0],48,48,1)\n",
    "test_images = np.array(test_images).reshape(test_images.shape[0],48,48,1)\n",
    "val_images = np.array(val_images).reshape(val_images.shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alphas = [alpha/1000 for alpha in range(1, 52, 5)]\n",
    "learning_rates = [learning_rate/1000000 for learning_rate in range(50,160,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_configs = []\n",
    "for leaky_alpha in leaky_alphas:\n",
    "    for learning_rate in learning_rates:\n",
    "        param_configs.append((leaky_alpha, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_param_configs = sample(param_configs, sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_train(param_config: tuple, param_results: dict, epochs: int):\n",
    "    print(f'Leaky Alpha: {param_config[0]}, Learning Rate: {param_config[1]}')\n",
    "    cnn_input = Input(shape=(48,48,1))\n",
    "    x = Conv2D(224, kernel_size=6, activation=LeakyReLU(alpha=param_config[0]))(cnn_input)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(448, kernel_size=2, activation=LeakyReLU(alpha=param_config[0]))(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    cnn_output = Dense(7, activation='softmax')(x)\n",
    "    \n",
    "    cnn = Model(inputs=[cnn_input], outputs=cnn_output)\n",
    "    optimizer = optimizers.Adam(learning_rate=param_config[1])\n",
    "    cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    cnn_history = cnn.fit(train_images, train_labels, validation_data=(test_images, test_labels),\n",
    "                          batch_size=200, epochs=epochs)\n",
    "    \n",
    "    val_accuracy = cnn_history.history['val_accuracy'][-1]\n",
    "    avg_accuracy_diff = np.mean(np.array(cnn_history.history['val_accuracy'][epochs-6:epochs])-np.array(cnn_history.history['accuracy'][epochs-6:epochs]))\n",
    "    param_results[param_config] = (val_accuracy, avg_accuracy_diff)\n",
    "    \n",
    "    del cnn_input\n",
    "    del cnn_output\n",
    "    del cnn\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "    print(f'Average Accuracy Difference: {round(avg_accuracy_diff,3)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_config in sampled_param_configs:\n",
    "    network_train(param_config, param_results, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alphas = []\n",
    "learning_rates = []\n",
    "max_val_accuracies = []\n",
    "avg_val_differences = []\n",
    "for param_result in sampled_param_configs:\n",
    "    leaky_alphas.append(param_result[0])\n",
    "    learning_rates.append(param_result[1])\n",
    "    max_val_accuracies.append(param_results[param_result][0])\n",
    "    avg_val_differences.append(param_results[param_result][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alphas, learning_rates, max_val_accuracies, avg_val_differences = pd.DataFrame(leaky_alphas).rename(columns={0:'leaky_alpha'}), pd.DataFrame(learning_rates).rename(columns={0:'learning_rate'}), pd.DataFrame(max_val_accuracies).rename(columns={0:'max_val_accuracy'}), pd.DataFrame(avg_val_differences).rename(columns={0:'avg_val_difference'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat([leaky_alphas, learning_rates, max_val_accuracies, avg_val_differences], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df[['max_val_accuracy', 'avg_val_difference']] = pd.DataFrame(hyperparameter_scaler.fit_transform(plot_df[['max_val_accuracy', 'avg_val_difference']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scatter(plot_df['leaky_alpha']*100, plot_df['learning_rate']*10000,\n",
    "           c=plot_df['max_val_accuracy'],\n",
    "           s=plot_df['avg_val_difference']*500)\n",
    "pl.legend()\n",
    "pl.colorbar()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alpha = 2.1/100\n",
    "learning_rate = 1.25/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tuned training parameters: learning_rate = {learning_rate}, leaky_alpha = {leaky_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
