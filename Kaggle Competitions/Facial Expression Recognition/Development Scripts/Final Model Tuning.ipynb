{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, Conv2D, Flatten, MaxPooling2D, AveragePooling2D, LeakyReLU, concatenate\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow import clip_by_value\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "from random import sample\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\train_images.parquet').read().to_pandas()\n",
    "test_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\test_images.parquet').read().to_pandas()\n",
    "val_images = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Images\\\\val_images.parquet').read().to_pandas()\n",
    "\n",
    "train_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\train_labels.parquet').read().to_pandas()\n",
    "test_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\test_labels.parquet').read().to_pandas()\n",
    "val_labels = pq.ParquetDataset('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Training Data\\\\Labels\\\\val_labels.parquet').read().to_pandas()\n",
    "\n",
    "train_images = np.array(train_images).reshape(train_images.shape[0],48,48,1)\n",
    "test_images = np.array(test_images).reshape(test_images.shape[0],48,48,1)\n",
    "val_images = np.array(val_images).reshape(val_images.shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.concatenate([train_images, test_images])\n",
    "train_labels = np.concatenate([train_labels, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_input = Input(shape=(48,48,1))\n",
    "x = Conv2D(224, kernel_size=6, activation=LeakyReLU(alpha=0.021))(cnn_input)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(448, kernel_size=2, activation=LeakyReLU(alpha=0.021))(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "cnn_output = Dense(7, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(inputs=[cnn_input], outputs=cnn_output)\n",
    "optimizer = optimizers.Adam(learning_rate=0.000125)\n",
    "cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn.fit(train_images, train_labels, validation_data=(val_images, val_labels),\n",
    "                      batch_size=200, epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Models\\\\Convolutional_Fully Trained\\\\Model.h5')\n",
    "with open('C:\\\\Users\\\\rbush\\\\Documents\\\\Projects\\\\Computer Vision\\\\Facial Expressions\\\\Models\\\\Convolutional_Fully Trained\\\\Training History.pkl','wb') as outfile:\n",
    "    pickle.dump(cnn_history.history, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predicted_categories = np.argmax(cnn.predict(val_images), axis=1)\n",
    "val_labels = np.argmax(np.array(val_labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cm = confusion_matrix(cnn_predicted_categories, val_labels, labels=[0,1,2,3,4,5,6])\n",
    "cnn_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_classifications = 0\n",
    "individual_classification_dict = {0:[0,0],\n",
    "                                  1:[0,0],\n",
    "                                  2:[0,0],\n",
    "                                  3:[0,0],\n",
    "                                  4:[0,0],\n",
    "                                  5:[0,0],\n",
    "                                  6:[0,0]}\n",
    "for pred in range(cnn_predicted_categories.shape[0]):        \n",
    "    # If the correct classification is Angry, tally the observation and determine whether the model was correct\n",
    "    if val_labels[pred] == 0:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Disgust, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 1:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Fear, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 2:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Happy, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 3:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Sad, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 4:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Surprise, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 5:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1\n",
    "            \n",
    "    # If the correct classification is Neutral, tally the observation and determine whether the model was correct        \n",
    "    elif val_labels[pred] == 6:\n",
    "        individual_classification_dict[val_labels[pred]][0] += 1\n",
    "        if cnn_predicted_categories[pred] == val_labels[pred]:\n",
    "            individual_classification_dict[val_labels[pred]][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0:'Angry',\n",
    "                1:'Disgust',\n",
    "                2:'Fear',\n",
    "                3:'Happy',\n",
    "                4:'Sad',\n",
    "                5:'Surprise',\n",
    "                6:'Neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    emotion_accuracy = individual_classification_dict[i][1]/individual_classification_dict[i][0]\n",
    "    emotion = emotion_dict[i]\n",
    "    print(f'{emotion} accuracy: {round(100*emotion_accuracy,1)}% ({individual_classification_dict[i][0]} images)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_accuracy = (cnn_cm[0,0]+cnn_cm[1,1]+cnn_cm[2,2]+cnn_cm[3,3]+cnn_cm[4,4]+cnn_cm[5,5]+cnn_cm[6,6])/np.sum(cnn_cm)\n",
    "print(f'Conv-Net Accuracy: {round(100*cnn_accuracy,1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
